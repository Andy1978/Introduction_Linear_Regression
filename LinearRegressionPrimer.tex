\documentclass[ngerman, 12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{nicefrac}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage[left=3cm,right=4cm,top=3cm,bottom=3cm]{geometry}
%\usepackage[]{mathpazo,palatino}
\usepackage[]{fourier}
\usepackage{microtype}
\usepackage{tikz}


% body:            Palatino 10pt
% section titles:  Palatino Bold
% formulas:        Euler-VM
%\usepackage{palatino}
%\usepackage{eulervm}
%\linespread{1.08}  % Palatino needs more leading

% body:            CM Roman 11pt
% section titles:  CM Sansserif Demibold Condensed
% formulas:        CM Math
%\usepackage{ccfonts}
%\setkomafont{sectioning}{\fontfamily{cmss}\fontseries{sbc}\selectfont}


\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{cancel}


% http://www.latex-community.org/forum/viewtopic.php?f=46&t=21038

\usepackage[bookmarksopen=true,pdfsubject={Statistik},pdftitle={Statistik},pdfauthor={Uwe Ziegenhagen},linkcolor=blue,citecolor=blue,urlcolor=blue,colorlinks=true,pdfproducer={PDFLaTeX},pdfcreator={LaTeX 2e},pdftex,backref]{hyperref}

\usepackage{amsmath,amstext}
\usepackage{attachfile}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\def\qs{\text{QS}(a,b)}
\def\sm{\sum\limits_{i=1}^{n}}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\E}{E}

\title{Herleitung der Parameter-Gleichungen für die einfache lineare Regression}
\subtitle{-- DRAFT --}
\author{Uwe Ziegenhagen}

\begin{document}

\maketitle

\subsubsection*{Historie}

\begin{description}
\item[v1.0] 16.03.2009, erste Version hochgeladen
\item[v2.0] 02.03.2013, einen Vorzeichenfehler beseitigt, diverse Gleichungen und Erläuterungen zum besseren Verständnis hinzugefügt.
\item [v3.0a] auf Github gewechselt, Metapost gegen TikZ getauscht, einige Erläuterungen verbessert, \LaTeX\ Code aufgeräumt
\end{description}

\section{Einführung}

Aus der Wikipedia\footnote{\url{https://de.wikipedia.org/wiki/Lineare_Regression}, Abruf: 24.06.2018}: 

\begin{quote}
\enquote{Die lineare Regression, die einen Spezialfall des allgemeinen Konzepts der Regressionsanalyse darstellt, ist ein statistisches Verfahren, mit dem versucht wird, eine beobachtete abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären. Das Beiwort \enquote{linear} ergibt sich dadurch, dass die abhängige Variable eine Linearkombination der Regressionskoeffizienten darstellt (aber nicht notwendigerweise der unabhängigen Variablen). Der Begriff Regression bzw. Regression zur Mitte wurde vor allem durch den Statistiker Francis Galton geprägt. 
}\end{quote}

Allgemein wird eine metrische Variable $Y$ betrachtet, die von ein oder mehreren Variablen $X_i$ abhängt. $Y$ nennt man die \enquote{abhängige Variable}, die $X_i$ sind die \enquote{unabhängigen Variablen}.  Im eindimensionalen Fall -- wenn es nur eine $X$-Variable gibt -- spricht man von einer einfachen linearen Regressionsanalyse, in höheren Dimensionen von der multiplen Regressionsanalyse.


\section{Einfache linerare Regression}

Im folgenden nutzen wir die Werte aus Tabelle \ref{tab:werte}, um die einfache lineare Regression zu erklären.

\begin{center}
\begin{tabular}{cc} \toprule[2pt]
$X$-Wert & $Y$-Wert \\ \midrule
        1 &1\\
        2 &3\\
        3 &2\\
        4 &5\\
        5 &4\\ \bottomrule[2pt]
\end{tabular}
\end{center}
\captionof{table}{Tabelle mit Wertepaaren}\label{tab:werte}\vspace*{1em}

Stellt man die Punkte in einem Streu-Diagramm wie in Abbildung \ref{fig:scatter} dar, so erkennt man dass mit steigendem Wert von $X$ die Werte von $Y$ ebenfalls steigen.

\begin{center}
\vspace*{1em}\begin{tikzpicture}[scale=1.5,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

%\draw[blue, very thick](0,1.5)--(6,3.3);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Scatterplot zur Darstellung der X-Y Wertepaare}\label{fig:scatter}\vspace*{1em}
\end{center}

Wenn wir den Zusammenhang dieser Punkte mittels Gerade modellieren wollen, unterstellen wir ein Modell der Form: 
   
\begin{equation}
	 Y_i = b + a \cdot x_i + \epsilon_i
\end{equation}
   
$b$ ist dabei der Achsenabschnitt, also der Punkt $(0,b)$, an dem die X-Achse geschnitten wird. $a$ hingegen ist der Parameter für die Steigung der Regressionsgeraden. $a$ und $b$ sind für unsere fünf Wertepaare zu bestimmen. ($\epsilon_i$ steht für die Fehler, den wir bei der Modellierung machen, darauf kommen wir gleich noch zu sprechen). 

Wir können wir nun die Regressionsgerade durch die Punkte zeichnen? Abbildung~\ref{fig:geraden} zeigt zwei Beispiele für eher zufällige Regressionsgeraden. Im linken Plot erkennt man sehr deutlich, dass die Gerade nicht zu unseren Punkten passen kann, sie zeigt in die falsche Richtung und unterstellt damit, dass mit steigendem $X$ die Werte für $Y$ sinken. Im rechten Plot stimmt die Richtung, die Gerade sieht schon \enquote{recht gut} aus. 

\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\node  [below] (suba) at (3.5,0){(a)};

\draw[blue, very thick](0,6)--(6,2);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (subb) at (3.5,0){(b)};

\draw[blue, very thick](0,1)--(6,5);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden}

Da aber eine Einschätzung wie \enquote{recht gut} nicht wirklich mathematisch exakt ist, müssen wir diesen Punkt ein wenig genauer betrachten.

Betrachten wir dazu Abbildung \ref{fig:geraden2}. Hier wurden auf der blauen Geraden die Punkte in grün markiert, die die Regressionsgleichung für den jeweiligen Wert von $X$ vorhersagt, außerdem wurden die jeweiligen Abstände zwischen dem wahren $Y$-Wert und dem geschätzten $Y$-Wert (den wir ab jetzt $\hat Y$ nennen) markiert. 

\begin{tikzpicture}[scale=1,
    pred/.style={circle, minimum size = 0.05cm,blue,fill=green},
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);


\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,6)--(6,2);


\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,5.3333){};
\node  [pred] (Bp) at (2,4.6666){};
\node  [pred] (Cp) at (3,4){};
\node  [pred] (Dp) at (4,3.3333){};
\node  [pred] (Ep) at (5,2.6666){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\node  [below] (suba) at (3.5,0){(a)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);
\end{tikzpicture}\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
    pred/.style={circle, minimum size = 0.05cm,blue,fill=green},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,1)--(6,5);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,1.6666){};
\node  [pred] (Bp) at (2,2.3333){};
\node  [pred] (Cp) at (3,3){};
\node  [pred] (Dp) at (4,3.6666){};
\node  [pred] (Ep) at (5,4.3333){};

\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);


\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (subb) at (3.5,0){(b)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden2}

Wenn wir beide Grafiken betrachten, so ist schnell sichtbar, dass die Regressionsgerade im linken Bild deutlich schlechter ist als die Regressionsgerade im rechten Bild: die Summe der Abstände zwischen den wahren, roten, Punkten und den durch die Gerade geschätzten Punkten ist viel größer.


Aus dieser Tatsache lässt sich ein sehr wichtiger Schluss ziehen: wenn wir diese Summe der Abstände minimieren könnten, würden wir die optimale Gerade erhalten. Als Gleichung:

\begin{equation}
S = \sm y_i - \hat{y}_i
\end{equation}\label{eq:abstaende}

In Worten: S ist die Summe der Differenzen von wahrem und geschätzten $y$-Wert.

Es hat sich jedoch herausgestellt, dass es sinnvoll ist, nicht die einfache Summe der Abstände zu minimieren, sondern die Summe der quadrierten Abstände. 

Daher wird aus Gleichung \ref{eq:abstaende} die folgende Gleichung:

\begin{equation}
QS = \left(\sm y_i - \hat{y}_i\right)^2
\end{equation}\label{eq:qabstaende}

Diese Quadratsumme der Abweichungen ist nur abhängig von den Parametern $a$ und $b$ der Regressionsgleichung, daher können wir schreiben:

\begin{equation}
QS(a,b) = \left(\sm y_i - \hat{y}_i\right)^2
\end{equation}\label{eq:qabstaende}


%Die Vorgehensweise bei der linearen Regression veranschaulicht folgende Grafik. Gegeben sind Wertepaare $x_i,y_i$, als schwarze Punkte eingezeichnet. Grün sind die Werte $(\hat{x},\hat{y})$ die durch die lineare Regressionsfunktion errechnet werden. Die roten Linien symbolisieren die Abweichungen\footnote{Es ist egal, ob man $y_i-\hat{y}_i$ oder $\hat{y}_i-y_i$ schreibt, durch die Quadrierung heben sich eventuelle negative Vorzeichen auf.}  $e_i=y_i-\hat{y}_i$ dieser durch die Gleichung bestimmten Punkte von den wahren Punkten. Aufgabe bei der Bestimmung der Parameter ist es nun, $a$ und $b$ so zu wählen, dass die Summe QS der quadrierten Abweichungen -- also $\sum_{i=1}^n (y_i-\hat{y}_i)^2$ -- minimal wird. 

%\begin{center}
%\includegraphics[width=0.75\textwidth]{linreg_metapost.1}
%\end{center}

Im Folgenden werden wir diese Funktion differenzieren, um die Gleichungen für die optimalen $a$ und $b$ zu ermitteln. 

\section{Herleitung der Parameter-Gleichungen}


\begin{align}\label{eq:1}
\qs	&= \sum_{i=1}^{n} e_i^2 = \sm \left(y_i - \hat{y}_i\right)^2\\
								&= \sm \Big(y_i - [ax_i+b]\Big)^2 \label{eq:2}
\end{align}

Da wir die optimalen Werte für die Minimierung dieser Quadratsumme erhalten wollen, bilden wir die partiellen Ableitungen nach $a$ und $b$. Vorher können wir jedoch Gleichung \ref{eq:1} vereinfachen. Mit Hilfe der 2. Binomischen Formel\footnote{\quad 2. Binomische Formel: \((s-t)^2 = s^2 -2st + t^2\)} lösen wir Gleichung \ref{eq:2} auf:

\begin{equation}
	\qs = \sm \Big(y_i^2 - 2y_i (ax_i+b) + (ax_i +b)^2\Big)
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel\footnote{\quad 1. Binomische Formel: \((s+t)^2 = s^2 +2st + t^2\)} entspricht, lösen wir auch diesen auf und vereinfachen:

\begin{equation}
	\qs = \sm \Big(y_i^2 - 2ax_iy_i - 2by_i + a^2x_i^2 + 2abx_i + b^2\Big)\label{eq:simp}
\end{equation}

Ausgehend von Gleichung \ref{eq:simp} bilden wir jetzt die partiellen Ableitungen nach $a$ und $b$:

\begin{align}
\frac{\partial \qs}{\partial a}	&= \sm (-2x_iy_i + 2ax_i^2 + 2bx_i) \label{eq:parta1}\\
								&= 2 \sm x_i(-y_i + ax_i + b) \label{eq:parta2} 
\end{align}

\begin{align}
\frac{\partial \qs}{\partial b}	&= \sm (-2y_i+2ax_i +2b) \label{eq:partb1}\\
								&= 2 \sm (ax_i + b -y_i) \label{eq:partb2}
\end{align}

Wenn wir Gleichung \ref{eq:partb2} nullsetzen und auflösen, erhalten wir 

\begin{alignat}{3}
2\sm ax_i &+ 2\sm b &&- 2 \sm y_i \quad &&= 0 \\
2\sm ax_i &+ \enskip 2nb &&- 2 \sm y_i \quad &&= 0 \\ 
2nb &= 2 \sm y_i &&- 2\sm ax_i && 
\end{alignat}
%& 2 \sm y_i &&- 2\sm ax_i &&= 2nb

Auflösen nach $b$ (durch $2n$ teilen) gibt (zusammen mit der Tatsache, dass das arithmetische Mittel allgemein als $\bar{x}= \frac{1}{n} \sm x_i$ definiert ist):

\begin{align}
b &= \frac{\sm y_i}{n} - \frac{\sm ax_i}{n} \\
	&= \frac{1}{n} \sm y_i - a \frac{1}{n} \sm x_i \\
	&= \bar{y} - a \bar{x}
\end{align}

Setzen wir nun $b=\bar{y} - a \bar{x}$ in Gleichung \ref{eq:parta2} ein, erhalten wir

\begin{equation}
	2\sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big)=0
	\label{eq:einsetz}
\end{equation}

Durch Ausmultiplizieren und Vereinfachen ergibt sich:

\begin{align}
0 &= 	\sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big) \\
	&= \sm \big(ax_i^2+x_i(\bar{y}-a\bar{x})-x_iy_i \big) \\
     &= \sm \big(ax_i^2 + x_i\bar{y} - a\bar{x}x_i- x_iy_i \big) \\
     &= \sm \big(ax_i^2 - a\bar{x}x_i + x_i\bar{y} - x_iy_i \big) 
\end{align}

\begin{align}
     &= \sm \big( (ax_i^2 - a\bar{x}x_i) + x_i\bar{y} - x_iy_i \big) \\
     &= \sm \big( ax_i^2 - a\bar{x}x_i \big) + \sm x_i\bar{y} - \sm x_iy_i 
\end{align}

Jetzt subtrahiert man $\sm x_iy_i$ und addiert $\sm x_i\bar{y}$, um diese beiden Teile auf die andere Seite der Gleichung zu bekommen.

\begin{equation}
\sm (ax_i^2 -ax_i\bar{x}) = \sm x_iy_i - \sm x_i\bar{y} 
\end{equation}

Da $a$ konstant ist, können wir es vor die Klammer ziehen.

\begin{equation}
a \sm (x_i^2 -x_i\bar{x}) = \sm x_iy_i - \bar{y} \sm x_i 
\end{equation}

Jetzt teilen wir durch $\sm (x_i^2 -x_i\bar{x})$

%\begin{equation}
%a \dfrac{\cancel{\sm (x_i^2 -x_i\bar{x})}}{\cancel{\sm (x_i^2 -x_i\bar{x})}}=  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})}
%\end{equation}


\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})} 
\end{equation}

Aus der Definition des arithmetischen Mittels $\bar{x} = \dfrac{1}{n}\sum x_i$ folgt $\sm x_i= n\bar{x}$. Einsetzen ergibt 

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n \bar{x}}{ \sm \big( x_i^2 -  \sm x_i \bar{x} \big)}
\end{equation}


Jetzt zerlegen wir die Summe unter dem Bruchstrich in Einzelsummen und ziehen $\bar{x}$ vor das zweite Summenzeichen (Zur Erinnerung: konstanter Term!)

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n\bar{x}}{\sm x_i^2 - \bar{x} \sm x_i}
\end{equation}

Über alternative Formeln zu Varianz und Kovarianz\footnote{Verschiebungssatz: \\ $\cov(X,Y) = \E \left((X - \E(X))(Y - \E(Y)\right)) =\E(X,Y)- \E(X)\E(Y)$ \\ $\var(X)=\E\left(\left(X-\E(X)\right)^2\right)=\E(X^2)-\left(\E(X)\right)^2$} erhalten wir

\begin{equation}
a =  \dfrac{\sm x_iy_i - n\bar{x}\bar{y}}{\sm x_i^2 -n\bar{x}^2} = \dfrac{n \left(\frac{1}{n}\sm x_iy_i - \bar{x}\bar{y}\right)}{ n\left( \frac{1}{n}\sm x_i^2 -\bar{x}^2\right)}  = \dfrac{n\cov(x,y)}{n\var(x)} = \dfrac{\cov(x,y)}{\var(x)}
\end{equation}


\section{Beispiel}

Für unser Beispiel vom Anfang hier die numerische Bestimmung der Parameter. Für $\bar{x}$ erhalten wir $3$, für $\bar{y}=2.4$, die Summe der $(x-\bar{x})(y-\bar{y})$ ergibt $3$, die Summe der $(x-\bar{x})^2=10$. Durch Einsetzen dieser Werte erhalten wir dann als Parameterwert für $b$
1.5, als Parameterwert für $a$ 0.3, sodass die Formel unseres linearen Modells $$y=0.3\cdot x + 1.5$$ lautet.

\begin{center}
\begin{tabular}{r|cccccc} \hline \hline
& 1 & 2 & 3 & 4 & 5 & 6 \\
& $x$	&	$y$	&	$x-\bar{x}$	&	$y-\bar{y}$	&	$(x-\bar{x})(y-\bar{y})$	&	$(x-\bar{x})^2$	\\ \hline
1 & 1	&	1	&	-2	&	-1.4	&	\, 2.8	&	4	\\
2 & 2	&	3	&	-1	&	\, 0.6	&	-0.6	&	1	\\
3 & 3	&	2	&	0	&	-0.4	&	\, 0.0	&	0	\\
4 & 4	&	4	&	1	&	\, 1.6	&	\, 1.6	&	1	\\
5 & 5	&	2	&	2	&	-0.4	&	-0.8	&	4	\\  \hline
$\sum$ & 15 & 12 
\end{tabular}
\end{center}

\section{Quelldateien}

Dieses Dokument wurde mit \LaTeX, dem freien Textsatzsystem, erstellt.

\begin{tabular}{rl}
 \LaTeX & \attachfile{LinearRegressionPrimer.tex} \\
Metapost & \attachfile{linreg_metapost.mp} \\
Metapost (kompiliert) & \attachfile{linreg_metapost.1}
\end{tabular}

\end{document}


\begin{align}
2\sm ax_i + 2\sm b - 2 \sm y_i &= 0 \\
2\sm ax_i \enskip +\enskip  2nb - 2 \sm y_i &= 0 \\
2nb &= 2\sm y_i - 2 \sm ax_i 
\end{align}

\partial

\stackrel{!}{=}0 





    };












\begin{equation}
	\qs = \sm \Big( \overbrace{y_i^2}^{s^2} - \overbrace{2y_i (ax_i+b)}^{2st} + \overbrace{(ax_i +b)^2}^{t^2}\Big)^2
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel entspricht, l�sen wir auch diesen auf:

