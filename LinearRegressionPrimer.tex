\documentclass[ngerman, 12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{nicefrac}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage[left=3cm,right=4cm,top=3cm,bottom=3cm]{geometry}
%\usepackage[]{mathpazo,palatino}
\usepackage{fourier}
\usepackage{microtype}
\usepackage{tikz}

\usepackage{listings}

\lstset{literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
    {~}{{\textasciitilde}}1
}

\definecolor{hellgelb}{rgb}{1,1,0.8}
\definecolor{colKeys}{rgb}{0,0,1}
\definecolor{colIdentifier}{rgb}{0,0,0}
\definecolor{colComments}{rgb}{1,0,0}
\definecolor{colString}{rgb}{0,0.5,0}



\lstset{%
    float=hbp,%
    basicstyle=\ttfamily\small, %
    identifierstyle=\color{colIdentifier}, %
    keywordstyle=\color{colKeys}, %
    stringstyle=\color{colString}, %
    commentstyle=\color{colComments}, %
    columns=flexible, %
    tabsize=2, %
    frame=single, %
    extendedchars=true, %
    showspaces=false, %
    showstringspaces=false, %
    numbers=left, %
    numberstyle=\tiny, %
    breaklines=true, %
    backgroundcolor=\color{hellgelb}, %
    breakautoindent=true, %
    captionpos=b%
}



% body:            Palatino 10pt
% section titles:  Palatino Bold
% formulas:        Euler-VM
%\usepackage{palatino}
%\usepackage{eulervm}
%\linespread{1.08}  % Palatino needs more leading

% body:            CM Roman 11pt
% section titles:  CM Sansserif Demibold Condensed
% formulas:        CM Math
%\usepackage{ccfonts}
%\setkomafont{sectioning}{\fontfamily{cmss}\fontseries{sbc}\selectfont}


\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{cancel}


% http://www.latex-community.org/forum/viewtopic.php?f=46&t=21038

\usepackage[bookmarksopen=true,pdfsubject={Statistik},pdftitle={Statistik},pdfauthor={Uwe Ziegenhagen},linkcolor=blue,citecolor=blue,urlcolor=blue,colorlinks=true,pdfproducer={PDFLaTeX},pdfcreator={LaTeX 2e},pdftex,backref]{hyperref}

\usepackage{amsmath,amstext}
\usepackage{attachfile}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\def\qs{\text{QS}(a,b)}
\def\sm{\sum\limits_{i=1}^{n}}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\E}{E}

\title{Einführung in die lineare Regression}
\subtitle{-- DRAFT VERSION--}
\author{Uwe Ziegenhagen}

\begin{document}

\maketitle

\subsubsection*{Historie}

\begin{description}
\item[v1.0] 16.03.2009, erste Version hochgeladen
\item[v2.0] 02.03.2013, einen Vorzeichenfehler beseitigt, diverse Gleichungen und Erläuterungen zum besseren Verständnis hinzugefügt.
\item [v3.0] auf Github gewechselt, Metapost gegen TikZ getauscht, einige Erläuterungen verbessert, \LaTeX\ Code aufgeräumt
\end{description}

\section{Einführung}

Aus der Wikipedia\footnote{\url{https://de.wikipedia.org/wiki/Lineare_Regression}, Abruf: 24.06.2018}: 

\begin{quote}
\enquote{Die lineare Regression, die einen Spezialfall des allgemeinen Konzepts der Regressionsanalyse darstellt, ist ein statistisches Verfahren, mit dem versucht wird, eine beobachtete abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären. Das Beiwort \enquote{linear} ergibt sich dadurch, dass die abhängige Variable eine Linearkombination der Regressionskoeffizienten darstellt (aber nicht notwendigerweise der unabhängigen Variablen). Der Begriff Regression bzw. Regression zur Mitte wurde vor allem durch den Statistiker Francis Galton geprägt. 
}\end{quote}

Allgemein wird eine metrische Variable $Y$ betrachtet, die von ein oder mehreren Variablen $X_i$ abhängt. $Y$ nennt man daher auch die \enquote{abhängige Variable} und die $X_i$ die \enquote{unabhängigen Variablen}.  Im eindimensionalen Fall -- wenn es nur eine $X$-Variable gibt -- spricht man von einer einfachen linearen Regression, in höheren Dimensionen von der multiplen Regression.

\section{Einfache lineare Regression}

Im folgenden nutzen wir die Werte aus Tabelle \ref{tab:werte}, um an ihnen die einfache lineare Regression zu erklären.

\begin{center}
\begin{tabular}{cc} \toprule[2pt]
$X$-Wert & $Y$-Wert \\ \midrule
        1 &1\\
        2 &3\\
        3 &2\\
        4 &5\\
        5 &4\\ \bottomrule[2pt]
\end{tabular}
\end{center}
\captionof{table}{Tabelle mit Wertepaaren}\label{tab:werte}\vspace*{1em}

Stellt man die Punkte in einem Streu-Diagramm (auf englisch \enquote{Scatterplot}) wie in Abbildung \ref{fig:scatter} dar, so erkennt man dass mit steigendem Wert von $X$ die Werte von $Y$ ebenfalls steigen.

\begin{center}
\vspace*{1em}\begin{tikzpicture}[scale=1.15,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};

\node  [left] (y1) at (0,1){1};
\node  [left] (y2) at (0,2){2};
\node  [left] (y3) at (0,3){3};
\node  [left] (y4) at (0,4){4};
\node  [left] (y5) at (0,5){5};

%\draw[blue, very thick](0,0.8)--(5,4.6);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Scatterplot zur Darstellung der X-Y Wertepaare}\label{fig:scatter}\vspace*{1em}
\end{center}

Wenn wir den Zusammenhang dieser Punkte mittels Gerade (also \enquote{linear}) modellieren wollen, unterstellen wir ein Modell der Form: 
   
\begin{equation}
	 Y_i = b + a \cdot x_i + \epsilon_i
\end{equation}

\begin{center}
\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);
\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);
\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\draw[blue, very thick](0,1)--(6,4);
\draw[red, very thick](2,2)--(5,2);
\draw[green, very thick](5,2)--(5,3.5);
\draw[magenta, very thick](-0.1,0.1)--(-0.1,0.9);
\node  [left] (b) at (0,1){b};
\node  [right] (y) at (5,2.75){\(\Delta y\)};
\node  [below] (x) at (3.5,2){\(\Delta x\)};
\end{tikzpicture}
\captionof{figure}{Grafische Erläuterung}\label{fig:grafisch}
\end{center}

$b$ ist dabei der Achsenabschnitt, also der Punkt $(0,b)$, an dem die X-Achse geschnitten wird. $a$ hingegen ist der Parameter für die Steigung der Regressionsgeraden. $a$ und $b$ sind für unsere fünf Wertepaare zu bestimmen. ($\epsilon_i$ steht für die Fehler, den wir bei der Modellierung machen, darauf kommen wir später noch zu sprechen). 

Abbildung \ref{fig:grafisch} beschreibt diesen Zusammenhang grafisch: der Achsenabschnitt ist der Schnittpunkt der Geraden mit der Y-Achse, der Anstieg das Verhältnis von \(\Delta y\) zu \(\Delta x\).

Wir können wir nun die Regressionsgerade durch die Punkte zeichnen? Abbildung~\ref{fig:geraden} zeigt zwei Beispiele für beliebig gewählte Regressionsgeraden. Im linken Plot erkennt man sehr deutlich, dass die Gerade nicht zu unseren Punkten passt, sie zeigt in die falsche Richtung und unterstellt damit, dass mit steigendem $X$ die Werte für $Y$ sinken. Im rechten Plot stimmt die Richtung, die Gerade sieht schon \enquote{recht gut} aus. 

\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\node  [below] (suba) at (3.5,0){(a)};

\draw[blue, very thick](0,6)--(6,2);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (subb) at (3.5,0){(b)};

\draw[blue, very thick](0,1)--(6,5);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden}

Da aber eine Einschätzung wie \enquote{recht gut} nicht wirklich mathematisch exakt ist, müssen wir diesen Punkt ein wenig genauer betrachten.

Betrachten wir dazu Abbildung \ref{fig:geraden2}. Hier wurden auf der blauen Geraden die Punkte in grün markiert, die die Regressionsgleichung für den jeweiligen Wert von $X$ vorhersagt, außerdem wurden die jeweiligen Abstände zwischen dem wahren $Y$-Wert und dem geschätzten $Y$-Wert (den wir ab jetzt $\hat Y$ nennen) markiert. 

\begin{tikzpicture}[scale=1,
    pred/.style={circle, minimum size = 0.04cm,blue,fill=green},
    punkt/.style={circle, minimum size = 0.04cm,blue,fill=red},
]

%\draw[help lines,yellow,thin] (0,0) grid (7,6);


\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,6)--(6,2);


\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,5.3333){};
\node  [pred] (Bp) at (2,4.6666){};
\node  [pred] (Cp) at (3,4){};
\node  [pred] (Dp) at (4,3.3333){};
\node  [pred] (Ep) at (5,2.6666){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};
\node  [below] (suba) at (3.5,0){(a)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);
\end{tikzpicture}\begin{tikzpicture}[scale=1,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
    pred/.style={circle, minimum size = 0.05cm,blue,fill=green},
]

%\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\draw[blue, very thick](0,1)--(6,5);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [pred] (Ap) at (1,1.6666){};
\node  [pred] (Bp) at (2,2.3333){};
\node  [pred] (Cp) at (3,3){};
\node  [pred] (Dp) at (4,3.6666){};
\node  [pred] (Ep) at (5,4.3333){};

\draw[magenta] (A) -- (Ap);
\draw[magenta] (B) -- (Bp);
\draw[magenta] (C) -- (Cp);
\draw[magenta] (D) -- (Dp);
\draw[magenta] (E) -- (Ep);


\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (subb) at (3.5,0){(b)};

%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Zwei Regressionsgeraden}\label{fig:geraden2}

Wenn wir beide Grafiken betrachten, so ist schnell sichtbar, dass die Regressionsgerade im linken Bild deutlich schlechter ist als die Regressionsgerade im rechten Bild: die Summe der Abstände zwischen den wahren, roten, Punkten und den durch die Gerade geschätzten Punkten ist viel größer.


Aus dieser Tatsache lässt sich ein sehr wichtiger Schluss ziehen: wenn wir diese Summe der Abstände minimieren könnten, würden wir die optimale Gerade erhalten. Als Gleichung:

\begin{equation}
S = \sm y_i - \hat{y}_i
\end{equation}\label{eq:abstaende}

In Worten: S ist die Summe aller Differenzen von wahrem und geschätzten $y$-Wert.

Es hat sich als mathematisch sinnvoll herausgestellt, nicht einfach die Summe der Abstände zu minimieren, sondern die Summe der \textit{quadrierten} Abstände. Es lässt sich nicht nur leicht damit rechnen, der Kleinste-Quadrate-Schätzer ist auch -- sofern die  Annahmen des klassischen linearen Regressionsmodells nicht verletzt sind -- BLUE (\enquote{Best Linear Unbiased Estimator}). Dazu später mehr\ldots

Aus Gleichung \ref{eq:abstaende} wird jetzt -- da wir ja die Quadratsumme minimieren wollen -- die folgende Gleichung:

\begin{equation}
QS = \left(\sm y_i - \hat{y}_i\right)^2
\end{equation}\label{eq:qabstaende}

Diese Quadratsumme der Abweichungen ist nur abhängig von den Parametern \(a\) und \(b\) der Regressionsgleichung, daher können wir schreiben:

\begin{equation}
QS(a,b) = \left(\sm y_i - \hat{y}_i\right)^2
\end{equation}\label{eq:qabstaende2}


%Die Vorgehensweise bei der linearen Regression veranschaulicht folgende Grafik. Gegeben sind Wertepaare $x_i,y_i$, als schwarze Punkte eingezeichnet. Grün sind die Werte $(\hat{x},\hat{y})$ die durch die lineare Regressionsfunktion errechnet werden. Die roten Linien symbolisieren die Abweichungen\footnote{Es ist egal, ob man $y_i-\hat{y}_i$ oder $\hat{y}_i-y_i$ schreibt, durch die Quadrierung heben sich eventuelle negative Vorzeichen auf.}  $e_i=y_i-\hat{y}_i$ dieser durch die Gleichung bestimmten Punkte von den wahren Punkten. Aufgabe bei der Bestimmung der Parameter ist es nun, $a$ und $b$ so zu wählen, dass die Summe QS der quadrierten Abweichungen -- also $\sum_{i=1}^n (y_i-\hat{y}_i)^2$ -- minimal wird. 

%\begin{center}
%\includegraphics[width=0.75\textwidth]{linreg_metapost.1}
%\end{center}

Im Folgenden werden wir diese Funktion partiell ableiten, um die Gleichungen für die optimalen \(a\) und \(b\) zu ermitteln. 

\section{Herleitung der Parameter-Gleichungen}

Wir schreiben Gleichung \ref{eq:qabstaende} nochmals auf und ersetzen \(\hat y\) durch die Modellgleichung:

\begin{align}\label{eq:1}
\qs	&  = \sm \left(y_i - \hat{y}_i\right)^2\\
								&= \sm \Big(y_i - [ax_i+b]\Big)^2 \label{eq:2}
\end{align}

Da wir die optimalen Werte für die Minimierung dieser Quadratsumme erhalten wollen, bilden wir die partiellen Ableitungen nach $a$ und $b$. Vorher können wir jedoch Gleichung \ref{eq:1} vereinfachen. Mit Hilfe der 2. Binomischen Formel\footnote{\quad 2. Binomische Formel: \((s-t)^2 = s^2 -2st + t^2\)} lösen wir Gleichung \ref{eq:2} auf:

\begin{equation}
	\qs = \sm \Big( \overbrace{y_i^2}^{s^2} - \overbrace{2y_i (ax_i+b)}^{-2st} + \overbrace{(ax_i +b)^2}^{t^2}\Big)
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel\footnote{\quad 1. Binomische Formel: \((s+t)^2 = s^2 +2st + t^2\)} entspricht, lösen wir auch diesen auf und vereinfachen:

\begin{equation}
	\qs = \sm \Big(y_i^2 - 2ax_iy_i - 2by_i + \overbrace{a^2x_i^2}^{s^2} + \overbrace{2abx_i}^{2st} + \overbrace{b^2}^{t^2}\Big)\label{eq:simp}
\end{equation}

Ausgehend von Gleichung \ref{eq:simp} bilden wir jetzt die partiellen Ableitungen nach \(a\) und \(b\). 

\begin{align}
\frac{\partial \qs}{\partial a}	&= \sm (-2x_iy_i + 2ax_i^2 + 2bx_i) \label{eq:parta1}\\
								&= 2 \sm x_i(-y_i + ax_i + b) \label{eq:parta2a} \\
								&= 2 \sm x_i(ax_i + b -y_i ) \label{eq:parta2} 
\end{align}

\begin{align}
\frac{\partial \qs}{\partial b}	&= \sm (-2y_i+2ax_i +2b) \label{eq:partb1}\\
								&= 2 \sm (ax_i + b -y_i) \label{eq:partb2}
\end{align}

Wenn wir Gleichung \ref{eq:partb2} nullsetzen und auflösen, erhalten wir 

\begin{alignat}{3}
2\sm ax_i &+ 2\sm b &&- 2 \sm y_i \quad &&= 0 \\
2\sm ax_i &+ \enskip 2nb &&- 2 \sm y_i \quad &&= 0 \\ 
2nb &= 2 \sm y_i &&- 2\sm ax_i && 
\end{alignat}
%& 2 \sm y_i &&- 2\sm ax_i &&= 2nb

Auflösen nach \(b\) (durch \(2n\) teilen) gibt (zusammen mit der Tatsache, dass das arithmetische Mittel allgemein als \(\bar{x}= \frac{1}{n} \sm x_i \) definiert ist):

\begin{align}
b &= \frac{\sm y_i}{n} - \frac{\sm ax_i}{n} \\
	&= \frac{1}{n} \sm y_i - a \frac{1}{n} \sm x_i \\
	&= \bar{y} - a \bar{x}
\end{align}

Setzen wir nun \(b=\bar{y} - a \bar{x}\) in Gleichung \ref{eq:parta2} ein, erhalten wir

\begin{equation}
	2\sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big)=0
	\label{eq:einsetz}
\end{equation}

Durch Ausmultiplizieren und Vereinfachen ergibt sich:

\begin{align}
  0 &= \sm x_i \big(ax_i + (\bar{y} - a \bar{x}) - y_i\big) \\
	&= \sm \big(ax_i^2+x_i(\bar{y}-a\bar{x})-x_iy_i \big) \\
     &= \sm \big(ax_i^2 + x_i\bar{y} - a\bar{x}x_i- x_iy_i \big) \\
     &= \sm \big(ax_i^2 - a\bar{x}x_i + x_i\bar{y} - x_iy_i \big) 
\end{align}

\begin{align}
     &= \sm \big( (ax_i^2 - a\bar{x}x_i) + x_i\bar{y} - x_iy_i \big) \\
     &= \sm \big( ax_i^2 - a\bar{x}x_i \big) + \sm x_i\bar{y} - \sm x_iy_i 
\end{align}

Jetzt addiert man \(\sm x_iy_i\) und subtrahiert \(\sm x_i\bar{y}\), um diese beiden Teile auf die andere Seite der Gleichung zu bekommen.

\begin{equation}
\sm (ax_i^2 -ax_i\bar{x}) = \sm x_iy_i - \sm x_i\bar{y} 
\end{equation}

Da \(a\) konstant ist, können wir es vor die Klammer ziehen.

\begin{equation}
a \sm (x_i^2 -x_i\bar{x}) = \sm x_iy_i - \bar{y} \sm x_i 
\end{equation}

Jetzt teilen wir durch \(\sm (x_i^2 -x_i\bar{x})\)

%\begin{equation}
%a \dfrac{\cancel{\sm (x_i^2 -x_i\bar{x})}}{\cancel{\sm (x_i^2 -x_i\bar{x})}}=  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})}
%\end{equation}


\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} \sm x_i}{\sm (x_i^2 -x_i\bar{x})} 
\end{equation}

Aus der Definition des arithmetischen Mittels $\bar{x} = \dfrac{1}{n}\sum x_i$ folgt $\sm x_i= n\bar{x}$. Einsetzen ergibt 

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n \bar{x}}{ \sm \big( x_i^2 -  \sm x_i \bar{x} \big)}
\end{equation}


Jetzt zerlegen wir die Summe unter dem Bruchstrich in Einzelsummen und ziehen \(\bar{x}\) vor das zweite Summenzeichen (Zur Erinnerung: konstanter Term!)

\begin{equation}
a =  \dfrac{\sm x_iy_i - \bar{y} n\bar{x}}{\sm x_i^2 - \bar{x} \sm x_i}
\end{equation}

Über Formeln zu Varianz und Kovarianz\footnote{Verschiebungssatz: \\ $\cov(X,Y) = \E \left((X - \E(X))(Y - \E(Y)\right)) =\E(X,Y)- \E(X)\E(Y)$ \\ $\var(X)=\E\left(\left(X-\E(X)\right)^2\right)=\E(X^2)-\left(\E(X)\right)^2$} erhalten wir

\begin{equation}
a =  \dfrac{\sm x_iy_i - n\bar{x}\bar{y}}{\sm x_i^2 -n\bar{x}^2} = \dfrac{n \left(\frac{1}{n}\sm x_iy_i - \bar{x}\bar{y}\right)}{ n\left( \frac{1}{n}\sm x_i^2 -\bar{x}^2\right)}  = \dfrac{n\cov(x,y)}{n\var(x)} = \dfrac{\cov(x,y)}{\var(x)}
\end{equation}

Damit haben wir die beiden Gleichungen hergeleitet, um die Regressionsgerade zu bestimmen:

\begin{equation}
b=\bar{y} - a \bar{x}
\end{equation}


\begin{equation}
a =  \dfrac{\cov(x,y)}{\var(x)}
\end{equation}

\clearpage


\section{Beispiel}

\begin{center}
\captionof{table}{Hilfstabelle}
\begin{tabular}{r|cccccc} \toprule
& 1 & 2 & 3 & 4 & 5 & 6 \\
& $x$	&	$y$	&	$x-\bar{x}$	&	$y-\bar{y}$	&	$(x-\bar{x})(y-\bar{y})$	&	$(x-\bar{x})^2$	\\ \midrule
1 & 1	&	1	&	-2	&	-2	&	4	&	4	\\
2 & 2	&	3	&	-1	&	0	&	0	&	1	\\
3 & 3	&	2	&	0	&	-1	&	0	&	0	\\
4 & 4	&	5	&	1	&	2	&	2	&	1	\\
5 & 5	&	4	&	2	&	1	&	2	&	4	\\  \midrule
$\sum$ & 15 & 15 & & & 8 & 10 \\ \bottomrule
\end{tabular}
\end{center}

%
%Für unser Beispiel vom Anfang hier die numerische Bestimmung der Parameter. Für \(\bar{x}\) erhalten wir 15/5 = 3, für \(\bar{y}\) ebenfalls 15/5 = 3, die Summe der \((x-\bar{x})(y-\bar{y})\) ergibt 7, die Summe der \((x-\bar{x})^2=10\). Durch Einsetzen dieser Werte erhalten wir dann als Parameterwert für \(b = 3 - a * 3\)
%1.5, als Parameterwert für \(a\) 0.3, sodass die Formel unseres linearen Modells \[y=0.3\cdot x + 1.5\] lautet.

Mit Hilfe der Werte aus der Tabelle lassen sich \(a\) und \(b\) einfach bestimmen. Hinweis: \(\bar x = 15/5 = 3\), \(\bar y = 15/5 = 3\)

\[ a = \frac{\frac{\sum_{i=1}^n \left(x_i-\bar x \right)\left(y_i-\bar y \right)}{5}}{ 
\frac{\sum_{i=1}^5 \left( x_i - \bar x \right)^2}{5}} = \frac{\sum_{i=1}^n\left(x_i-\bar x \right)\left(y_i-\bar y \right)}{\sum_{i=1}^n \left(x_i-\bar x \right)^2}  = \frac{8}{10} = 0.8 \]

Hinweis zu dieser Rechnung: Brüche werden dividiert, indem man mit dem Kehrwert multipliziert: \( \frac{\nicefrac{1}{5}}{\nicefrac{1}{5}}   = \nicefrac{1}{5} \cdot \nicefrac{5}{1}  = 1 \). Für die Berechnung von \(a\) braucht man die Anzahl der Beobachtungen also nicht mehr.

\[ b = \bar y - a\cdot \bar x = 3 - 0.8 \cdot 3 = 3 - 2.4 = 0.6 \]

Mit den gefundenen Werten für unsere beiden Parameter können wir jetzt die Regressionsgerade zeichnen:\vspace*{-11mm}

\begin{center}
\vspace*{1em}\begin{tikzpicture}[scale=0.95,
    punkt/.style={circle, minimum size = 0.05cm,blue,fill=red},
]

\draw[help lines,yellow,thin] (0,0) grid (7,6);

\draw [->,thick] (0,0) -- (7,0);
\draw [->,thick] (0,0) -- (0,6);

\node  [punkt] (A) at (1,1){};
\node  [punkt] (B) at (2,3){};
\node  [punkt] (C) at (3,2){};
\node  [punkt] (D) at (4,5){};
\node  [punkt] (E) at (5,4){};

\node  [below] (X) at (7,0){X};
\node  [left] (Y) at (0,6){Y};

\node  [below] (x1) at (1,0){1};
\node  [below] (x2) at (2,0){2};
\node  [below] (x3) at (3,0){3};
\node  [below] (x4) at (4,0){4};
\node  [below] (x5) at (5,0){5};
\node  [below] (x6) at (6,0){6};

\node  [left] (y1) at (0,1){1};
\node  [left] (y2) at (0,2){2};
\node  [left] (y3) at (0,3){3};
\node  [left] (y4) at (0,4){4};
\node  [left] (y5) at (0,5){5};

\draw[blue, very thick](0,0.8)--(6,5.4);
%\node[punkt] (A) at (9,1){};
%\node[draw] (B) at (8,4){B};
%\draw (A) -- (B);
\end{tikzpicture}
\captionof{figure}{Scatterplot mit Regressionsgerade}\label{fig:final}\vspace*{1em}
\end{center}

\section{Maße für die Güte der Linearen Regression}

In diesem Abschnitt möchte ich erläutern, wie man die Stärke des linearen Zusammenhangs und die Güte des linearen Modells bestimmt. Unsere Regressionsparameter \(a\) und \(b\) sind die optimalen Modellparameter für die von uns genutzten Daten, aber sie erklären nicht wie gut die Werte der unabhängigen Variablen die Werte unserer abhängigen Variablen erklären.

Schauen wir uns zuerst den Korrelationskoeffizienten \(r\) an: Der Bravais-Pearson-Koeffizient \(r\) misst die Stärke und Richtung des linearen Zusammenhangs zwischen zwei metrischen Variablen. Für \(r\) gibt es zwei Formeln:

\begin{equation}
r = 
\end{equation}

\begin{equation}
r = 
\end{equation}





Um hierüber eine Aussage zu treffen, nutzt man das sogenannte Bestimmtheitsmaß \(R^2\), je nach Literatur auch \enquote{Determinationskoeffizient} genannt. \(R^2\) ist ein Anteilswert, der die erklärte Varianz in das Verhältnis zur Gesamtvarianz setzt. Daraus folgt, dass \(R^2\) Werte zwischen 0 und 1 (0\% und 100\%) annehmen kann. 

Die Formel dafür lautet:

\begin{equation}
R^2 = \frac{\sum_{i=1}^n (\hat{y_i} - \bar{y})^2} {\sum_{i=1}^n (y_i - \bar{y})^2}
\end{equation}

Rechnen wir im nächsten Schritt mal \(R^2\) für unser Zahlenbeispiel aus:


\begin{center}
\captionof{table}{Hilfstabelle für die Berechnung von \(R^2\)}
\begin{tabular}{r|lllr} \toprule
i & \(y_i\)  & \(\hat{y_i}\) & \((\hat{y_i} - \bar{y})^2 \)&  \((y_i - \bar{y})^2\) \\ \midrule
1 & 1 &  1.4 & 2.56    & 4.0\\
2 & 3 &  2.2 &  0.64 & 0.0\\
3 & 2 &  3.0 & 0.0     & 1.0\\ 
4 & 5 &  3.8 & 0.64    & 4.0\\ 
5 & 4 &  4.6 & 2.56    & 1.0\\ \midrule
\(\sum\) & & & 6.4 & 10.0\\ \bottomrule
\end{tabular}
\end{center}

Damit ergibt sich

\begin{equation}
R^2 = \frac{6.4}{10.0} = 64 \%
\end{equation}


Zu beachten ist, dass \(R^2\) nur eine Aussage über den \textit{linearen} Zusammenhang trifft, für Daten mit einem nicht-linearen Zusammenhang ist es nicht geeignet.

\section{Berechnung mit dem Taschenrechner}

\subsection{Schultaschenrechner}

Moderne (Schul-)Taschenrechner haben alle entsprechende Funktionen eingebaut, um anhand von übergebenen Wertepaaren die Parameter $a$ und $b$ schnell zu bestimmen. Im Folgenden zeigen wir anhand eines Casio Schultaschenrechners vom Typ , wie es funktioniert. \marginpar{TODO!}



\section{Code-Beispiele}


\subsection{Python}

\lstinputlisting[language=Python,morekeywords={linregress}]{linreg-01.py}

\subsection{R}


\subsection{Microsoft Excel}















\section*{Quelldateien}

Dieses Dokument wurde mit \LaTeX, dem freien Textsatzsystem, erstellt. Die Quelldatei dieses Dokuments ist im PDF enthalten, klicken Sie einfach auf das Symbol. Sofern Ihr PDF-Betrachter Attachments unterstützt, sollten Sie auf die Quelldatei zugreifen können.

\begin{tabular}{rl}
  \LaTeX & \attachfile{LinearRegressionPrimer.tex} \\
  Python & \attachfile{linreg-01.py} \\
\end{tabular}



\end{document}


\begin{align}
2\sm ax_i + 2\sm b - 2 \sm y_i &= 0 \\
2\sm ax_i \enskip +\enskip  2nb - 2 \sm y_i &= 0 \\
2nb &= 2\sm y_i - 2 \sm ax_i 
\end{align}

\partial

\stackrel{!}{=}0 


\begin{equation}
	\qs = \sm \Big( \overbrace{y_i^2}^{s^2} - \overbrace{2y_i (ax_i+b)}^{2st} + \overbrace{(ax_i +b)^2}^{t^2}\Big)^2
\end{equation}

Da der Term \((ax_i +b)^2\) der 1. Binomischen Formel entspricht, l�sen wir auch diesen auf: